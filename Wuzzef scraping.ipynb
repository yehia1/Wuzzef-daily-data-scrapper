{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 352,
   "metadata": {},
   "outputs": [],
   "source": [
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.keys import Keys\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.support.ui import Select\n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "import time\n",
    "import pandas as pd\n",
    "from datetime import date\n",
    "from openpyxl import load_workbook\n",
    "import os\n",
    "import warnings\n",
    "\n",
    "warnings.simplefilter(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def main(page):\n",
    "#     src = page.content\n",
    "    \n",
    "#     soup = BeautifulSoup(src,'lxml')\n",
    "#     job_details = []\n",
    "#     job_list = soup.find_all(\"div\",{'class': 'css-1gatmva'})\n",
    "\n",
    "\n",
    "\n",
    "    \n",
    "#     for i in range(len(job_list)):\n",
    "#         job_title = job_list[i].find('div',{'class','css-laomuu'}).find('a').text.strip()\n",
    "#         # date_of_publish = job_list[i].find('div',{'class','css-do6t5g'}).text\n",
    "#         Company_name = job_list[i].find('div',{'class','css-d7j1kk'}).find('a').text.strip('- ')\n",
    "#         Location = job_list[i].find('span',{'class': 'css-5wys0k'}).text.strip()\n",
    "#         job_skills = job_list[i].find('div',{'class': 'css-y4udm8'}).contents[1].text\n",
    "#         links = 'https://wuzzuf.net'+job_list[i].find('a',{'class','css-o171kl'}).attrs['href'] \n",
    "\n",
    "\n",
    "#         job_details.append({\n",
    "#             'job_title':job_title,\n",
    "#             # 'date_of_publish':date_of_publish,\n",
    "#             'Company_name': Company_name,\n",
    "#             'Location':Location,\n",
    "#             'skills' :job_skills,\n",
    "#             'link':links\n",
    "#         })\n",
    "    \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#     columns = job_details[0].keys()\n",
    "#     with open('D:\\Coding\\Data anaylsis\\Web Scraping\\Wuzzef_jobs_details.csv','w',encoding= 'utf-8') as output :\n",
    "#         dict_writer = csv.DictWriter(output,columns)\n",
    "#         dict_writer.writeheader()\n",
    "#         dict_writer.writerows(job_details)\n",
    "#         print('file_created')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [],
   "source": [
    "def set_min_max_experience(driver,min:int = 0,max:int = 5):\n",
    "    \n",
    "    experience_section = driver.find_element(By.XPATH, \"//h3[contains(., 'Years of experience')]\")\n",
    "    experience_section.click()\n",
    "\n",
    "    time.sleep(1)\n",
    "\n",
    "    # Wait for the \"Min\" input field to appear\n",
    "    wait = WebDriverWait(driver, 10)\n",
    "    \n",
    "    # Locate the 'Min' field using the class 'css-1ph4zhu-placeholder'\n",
    "    min_input = wait.until(EC.presence_of_element_located((By.XPATH, \"//div[contains(@class,'css-1ph4zhu-placeholder') and text()='Min']/following::input[1]\")))\n",
    "    min_input.click()\n",
    "    min_input.clear()\n",
    "    min_input.send_keys(\"0\")\n",
    "    min_input.send_keys(Keys.ENTER)\n",
    "\n",
    "\n",
    "    # Wait for the results page to load\n",
    "    time.sleep(3)\n",
    "\n",
    "\n",
    "    # Step 3: Wait for and select the 'Max' input field\n",
    "    max_input = wait.until(EC.presence_of_element_located((By.XPATH, \"//div[contains(@class,'css-1ph4zhu-placeholder') and text()='Max']/following::input[1]\")))\n",
    "    max_input.click()\n",
    "    max_input.clear()\n",
    "    max_input.send_keys(\"5\")\n",
    "    max_input.send_keys(Keys.ENTER)\n",
    "\n",
    "    print(f\"Successfully selected the Years of experience Filter.\")\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [],
   "source": [
    "def select_date_posted_option(driver,option_text):\n",
    "    \"\"\"\n",
    "    Selects one of the options in the 'Date Posted' filter section on a webpage.\n",
    "    \n",
    "    Parameters:\n",
    "        option_text (str): The text of the option you want to select. \n",
    "                           Valid options: 'All', 'Past 24 hours', 'Past week', 'Past month'\n",
    "    \"\"\"\n",
    "    try:\n",
    "\n",
    "        # Step 1: Open the \"Date Posted\" section (if not already opened)\n",
    "        date_posted_section = driver.find_element(By.XPATH, \"//h3[contains(., 'Date Posted')]\")\n",
    "\n",
    "        date_posted_section.click()\n",
    "\n",
    "        driver.execute_script(\"arguments[0].scrollIntoView(true);\", date_posted_section)\n",
    "\n",
    "        \n",
    "        # Wait for the \"Min\" input field to appear\n",
    "        wait = WebDriverWait(driver, 10)\n",
    "\n",
    "        # Step 3: Find the radio button using the provided text (e.g., 'Past week')\n",
    "        option_xpath = f\"//span[text()='{option_text}']/preceding::input[1]\"\n",
    "        desired_option = wait.until(EC.presence_of_element_located((By.XPATH, option_xpath)))\n",
    "\n",
    "        # Step 4: Use JavaScript to click the radio option\n",
    "        driver.execute_script(\"arguments[0].click();\", desired_option)\n",
    "\n",
    "        print(f\"Successfully selected the '{option_text}' option.\")\n",
    "    \n",
    "    except Exception as e:\n",
    "        print(f\"An error occurred: {str(e)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 279,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_job_details(driver, job_data):\n",
    "    \"\"\"\n",
    "    Extracts job details from a list of job cards on a Wuzzuf-like job site and navigates through the job details page.\n",
    "\n",
    "    Parameters:\n",
    "    - driver: Selenium WebDriver instance.\n",
    "    - job_data: List to store the extracted job information. Each job's details will be appended as a dictionary.\n",
    "\n",
    "    Returns:\n",
    "    - None: The function adds the scraped job data to the job_data list.\n",
    "    \"\"\"\n",
    "\n",
    "    # Step 1: Find all job cards on the current page\n",
    "    job_cards = driver.find_elements(By.CLASS_NAME, 'css-1gatmva')\n",
    "    # Step 2: Iterate through each job card and extract the job details\n",
    "    for job_card in job_cards:\n",
    "        try:\n",
    "            # Find the job title link and its URL\n",
    "            job_title_link = job_card.find_element(By.CSS_SELECTOR, \"a.css-o171kl\")\n",
    "            job_url = job_title_link.get_attribute(\"href\")\n",
    "\n",
    "            # Open the job URL in a new tab\n",
    "            driver.execute_script(f\"window.open('{job_url}', '_blank');\")\n",
    "                \n",
    "            # Switch to the new tab where job details are displayed\n",
    "            driver.switch_to.window(driver.window_handles[-1])\n",
    "            # Optional: Add a delay to allow the page to load the job details\n",
    "            time.sleep(2)\n",
    "\n",
    "            # Step 3: Extract job details (adjust the CSS selectors as per the page's structure)\n",
    "            job_title = driver.find_element(By.CSS_SELECTOR, 'h1.css-f9uh36').text.strip()\n",
    "            # Extract company name and location using JavaScript\n",
    "            company_name = driver.execute_script(\"return document.querySelector('strong.css-9geu3q').textContent\").split('\\xa0')[-2][:-2]\n",
    "            location = driver.execute_script(\"return document.querySelector('strong.css-9geu3q').textContent\").split('\\xa0')[-1]\n",
    "            # Extract experience, salary, skills, description, and job requirements\n",
    "            experience = driver.find_element(By.CSS_SELECTOR, 'span.css-4xky9y').text.strip()\n",
    "            salary = driver.find_element(By.XPATH, \"//div[contains(@class, 'css-rcl8e5')]//span[contains(text(), 'Salary')]/following-sibling::span\").text\n",
    "            skills = driver.find_element(By.CSS_SELECTOR, 'div.css-s2o0yh').text.split()[3:]\n",
    "            job_description = driver.find_element(By.CSS_SELECTOR, 'div.css-1uobp1k').text.split('\\n')[1:]\n",
    "            try :\n",
    "                job_requirements = driver.find_element(By.CSS_SELECTOR, 'div.css-1t5f0fr').text.split('\\n')[1:]\n",
    "            except Exception as e:\n",
    "                job_requirements = None\n",
    "            # Append the extracted details to the job_data list\n",
    "            job_data.append({\n",
    "                \"Job Title\": job_title,\n",
    "                \"Company\": company_name,\n",
    "                \"Location\": location,\n",
    "                \"Job Link\": job_url,\n",
    "                \"Salary\": salary,\n",
    "                \"Experience\": experience,\n",
    "                \"Skills\": skills,\n",
    "                \"Job Requirements\": job_requirements,\n",
    "                \"Description\": job_description\n",
    "            })\n",
    "\n",
    "            # Close the job details tab\n",
    "            driver.close()\n",
    "            \n",
    "            # Step 4: Switch back to the original tab to continue extracting the next job\n",
    "            driver.switch_to.window(driver.window_handles[0])\n",
    "        except Exception as e:\n",
    "            # If any error occurs, print it for debugging\n",
    "            print(f\"Failed to extract details for job: {e}\")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 277,
   "metadata": {},
   "outputs": [],
   "source": [
    "def navigate_pages(driver, job_data):\n",
    "    \"\"\"\n",
    "    Navigates through the pages of job listings, extracts job details from each page,\n",
    "    and logs the current page number.\n",
    "\n",
    "    Args:\n",
    "        driver: Selenium WebDriver instance.\n",
    "        job_data: A list to append extracted job details.\n",
    "\n",
    "    Returns:\n",
    "        None\n",
    "    \"\"\"\n",
    "    page_index = 1  # Start with the first page\n",
    "\n",
    "    while True:\n",
    "        print(f\"Processing page {page_index}...\")  # Log the current page\n",
    "        \n",
    "        # Step 1: Extract job details from the current page\n",
    "        extract_job_details(driver, job_data)\n",
    "\n",
    "        try:\n",
    "            next_button = driver.find_element(By.XPATH, f\"//button[contains(@class,'css-zye1os') and .//a[contains(@href,'start={page_index}')]]\")\n",
    "            next_button.click()\n",
    "\n",
    "            # Optional: Add a delay to allow the next page to load\n",
    "            time.sleep(3)\n",
    "\n",
    "            # Step 4: Ensure the page has actually advanced (log the current URL to verify)\n",
    "            current_url = driver.current_url\n",
    "            print(f\"Successfully moved to page {page_index + 1}. URL: {current_url}\")\n",
    "\n",
    "            # Increment the page index\n",
    "            page_index += 1\n",
    "\n",
    "        except Exception as e:\n",
    "            # If the \"Next\" button is not found, we are on the last page\n",
    "            print(\"No more pages to navigate. Scraping complete.\")\n",
    "            break  # Exit the loop when there are no more pages\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 345,
   "metadata": {},
   "outputs": [],
   "source": [
    "def append_sheet_to_excel(df, file_name, sheet_name):\n",
    "    \"\"\"\n",
    "    Append a DataFrame to a new or existing sheet in an Excel file.\n",
    "    If the file doesn't exist, it creates a new file.\n",
    "    If the sheet doesn't exist, it creates a new sheet without touching existing sheets.\n",
    "\n",
    "    Args:\n",
    "    df (pd.DataFrame): DataFrame to write to Excel.\n",
    "    file_name (str): Path to the Excel file.\n",
    "    sheet_name (str): Sheet name to write data to.\n",
    "    \"\"\"\n",
    "    # Check if the file exists\n",
    "    if os.path.exists(file_name):\n",
    "        # Load the existing workbook\n",
    "        book = load_workbook(file_name)\n",
    "        # Use 'openpyxl' engine to preserve existing sheets\n",
    "        with pd.ExcelWriter(file_name, engine='openpyxl', mode='a', if_sheet_exists='overlay') as writer:\n",
    "            writer.book = book\n",
    "            # If the sheet doesn't exist, create it\n",
    "            if sheet_name not in writer.book.sheetnames:\n",
    "                df.to_excel(writer, sheet_name=sheet_name, index=False)\n",
    "\n",
    "                worksheet = writer.sheets[sheet_name]\n",
    "                \n",
    "                for row_num, job_link in enumerate(df[\"Job Link\"], start=worksheet.max_row - len(df)):\n",
    "                    worksheet.cell(row=row_num + 1, column=4).hyperlink = job_link\n",
    "            else:\n",
    "                # Get the last row in the existing data\n",
    "                startrow = writer.sheets[sheet_name].max_row\n",
    "                # Append the new data below the existing data\n",
    "                df.to_excel(writer, sheet_name=sheet_name, index=False, header=False, startrow=startrow)\n",
    "\n",
    "                worksheet = writer.sheets[sheet_name]\n",
    "                # Iterate over the rows in the DataFrame for adding hyperlinks\n",
    "                for row_num, job_link in enumerate(df[\"Job Link\"], start=worksheet.max_row - len(df)):  # Adjust start row\n",
    "                    worksheet.cell(row=row_num + 1, column=4).hyperlink = job_link  # Column 4 is for Job Link\n",
    "\n",
    "        print(f\"Data appended to '{sheet_name}' in '{file_name}'.\")\n",
    "    else:\n",
    "        # If the file doesn't exist, create a new one\n",
    "        with pd.ExcelWriter(file_name, engine='openpyxl') as writer:\n",
    "            df.to_excel(writer, sheet_name=sheet_name, index=False)\n",
    "        print(f\"File '{file_name}' created with new sheet '{sheet_name}'.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 353,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully selected the Years of experience Filter.\n",
      "Successfully selected the 'Past 24 hours' option.\n",
      "Processing page 1...\n",
      "No more pages to navigate. Scraping complete.\n",
      "Data appended to '2024-09-24' in 'Wuzzef_jobs_details.xlsx'.\n"
     ]
    }
   ],
   "source": [
    "def main(position:str):\n",
    "    driver = webdriver.Chrome()\n",
    "\n",
    "    driver.get(\"https://wuzzuf.net\")\n",
    "    search_input = driver.find_element(By.CLASS_NAME,'css-ukkbbr.e1n2h7jb1')\n",
    "\n",
    "    # Clear the search input if necessary and enter the job title (e.g., 'Data Analyst')\n",
    "    search_input.clear()\n",
    "    search_input.send_keys(position)\n",
    "\n",
    "    # Simulate pressing the ENTER key to search\n",
    "    search_input.send_keys(Keys.RETURN)\n",
    "\n",
    "    # Wait for the results page to load\n",
    "    time.sleep(5)\n",
    "    \n",
    "    set_min_max_experience(driver,min = 0,max =  5)\n",
    "\n",
    "    # Wait for the results page to load\n",
    "    time.sleep(3)\n",
    "    \n",
    "    # Example usage: Select 'Past week'\n",
    "    select_date_posted_option(driver,'Past 24 hours')\n",
    "    \n",
    "    time.sleep(5)\n",
    "\n",
    "    job_data = []\n",
    "    navigate_pages(driver,job_data)\n",
    "\n",
    "    df = pd.DataFrame(job_data)\n",
    "    append_sheet_to_excel(df, 'Wuzzef_jobs_details.xlsx', sheet_name=f'{date.today()}')\n",
    "\n",
    "main('Data Analyst')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
